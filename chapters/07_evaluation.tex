\chapter{Evaluation}
\label{ch:eval}


\section{Experimental Setup}\label{sec:experimental-setup}

We conducted our benchmarks on a Google Pixel 8 equipped with a Google Tensor G3 chip, comprising 1\,$\times$\,Cortex-X3 (2.91\,GHz), 4\,$\times$\,Cortex-A715 (2.37\,GHz), and 4\,$\times$\,Cortex-A510 (1.7\,GHz) cores, with Memory Tagging Extension (MTE) enabled.
As of the date of writing, this is the sole commercially available device featuring MTE.
To mitigate thermal throttling, we attached a cooling fan to the device.
Each performance test was run on each CPU type available on the Tensor G3 chip by pinning to a single respective core.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{c || c c c}
        \hline
        \textbf{Variant} & \textbf{Pointer Width} & \textbf{Memory Safety} & \textbf{MTE Bounds Checks} \\
        \hline
        wasm32     & 32\,bit & No  & No  \\
        wasm64     & 64\,bit & No  & No  \\
        mem-safety & 64\,bit & Yes & No  \\
        mte-bounds & 64\,bit & No  & Yes \\
        combined   & 64\,bit & Yes & Yes \\
        \hline
    \end{tabular}
    \caption{Benchmarking Variants}
    \label{tab:benchmark-variants}
\end{table}

\subsection{Benchmark Variants}
\label{subsec:benchmark-variants}
We ran the benchmarks from the PolyBench/C 3.2 suite~\cite{polybenchc}.

\section{Performance Overheads}
\label{sec:performance-overheads}

\begin{figure}[ht]
    \centering
    \input{plots/runtimes-all.pgf}
    \caption{PolyBench/C runtime overheads of different configurations described in \cref{tab:benchmark-variants}, normalized to wasm64.}
    \label{fig:runtime-overheads-combined}
\end{figure}

\Cref{fig:runtime-overheads-combined} illustrates the mean runtime overheads for PolyBench/C benchmarks for each CPU core available on the Tensor G3 chip.
We can see that, compared to wasm64, our memory safety extension has a mean overhead of 5.8 and 4.9\,\% on the two out-of-order high performance cores.
On the in-order Cortex-A510, we see an overhead of \todo{x}\,\%.
Generally we see that the overhead of bounds checks through the switch from wasm32 to wasm64 is lower on the out-of-order cores, as those can speculate bounds checks, while the in-order cores cannot.

When we replace software-based bounds checks with \ac{MTE} bounds checks, we see the overhead largely disappearing.
The remaining overhead can be explained through (1) the natural overhead that enabling \ac{MTE} sunc mode costs (see \cref{subsec:synchronous-and-asynchronous-mode}) and (2) that the linear memory needs to be tagged after allocation.
This overhead is especially noticeable for short-running modules that require large amounts of linear memory.

Combining both \ac{MTE} bounds and \ac{MTE} memory safety, we see a slight increase from just \ac{MTE} bounds, that is smaller than the jump from wasm64 to memory safety.
This is that we have already paid for the \ac{MTE} overhead, the overhead we are seeing here is the overhead resulting in additional tagging instructions.

We could decrease the overhead even further by switching to \ac{MTE} async mode, which poses a lower overhead, compared to sync mode (see \cref{subsec:synchronous-and-asynchronous-mode}).
However, this dramatically reduces the security guarantees provided by \ac{MTE}, as illegal writes and reads may become observable.
This disqualifies async \ac{MTE} for bounds checking of \ac{WASM} sandboxes, as attackers may carefully craft malicious code to escape their sandbox.
For the memory safety extension, users may decide the additional risk is worth the reduced overhead, e.g., when the memory safety extension is not used as a primary defense mechanism, but as a second layer or to find bugs in the wild.

\section{Memory Overheads}\label{sec:memory-overheads}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{./plots/mem-overhead}
    \caption{{PolyBench/C memory overheads of our prototype. (a) Baseline. (b) Relative overhead of wasm64 with memory safety features. (c) Bounds checks implemented using MTE.}{}}
    \label{fig:memory_overheads}
\end{figure*}

We measure the memory overhead with GNU time.

Memory tagging incurs overhead, particularly for small allocations due to the 16-byte alignment required for MTE.
The MTE backend further introduces overhead, as observed in \cref{fig:memory_overheads}, due to OS-level memory allocation requirements when MTE is active.
Enabling our memory safety mechanism results in an average overhead of 1\%, with a range of -0.67\% to 23.5\%.
In contrast, replacing bounds checks with MTE, which necessitates tagging the entire WebAssembly linear memory, leads to an average memory overhead of 0.26\%, with a range of -0.7\% to 1.4\%.


\section{Security Guarantees}\label{sec:security-guarantees}


\todo{insert evaluation here}

\section{MTE Performance evaluation}
\label{sec:mte-performance-evaluation}

We evaluated the performance of different characteristics of \ac{MTE} as implemented on the Tensor G3 chip.
All the programs used to measure results in this section are implemented in Rust and available on GitHub\footnote{\url{https://github.com/martin-fink/mte-stg-bench}}.
As our benchmarking library we used criterion\footnote{\url{https://github.com/bheisler/criterion.rs}}.
After each benchmarking run, we let the device cool down for 30\,seconds to prevent throttling.

\subsection{Tagging Primitives}
\label{subsec:tagging-primitives}

We evaluated the performance of the different types of instructions to set the tag for a segment of memory available in EL0 (user space) with the combinations described in \cref{tab:stg-instructions}.
Here, instruction refers to the instruction being used to set the tag and granule size refers to the size of memory being tagged with a single instruction.
The instructions \texttt{stzg} and \texttt{stgp} implicitly set the granule to zero, while we have to use an explicit memset for other instructions.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{| l || c | c |  c | c |}
        \hline
        \textbf{Variant} & \textbf{Instruction} & \textbf{Granule size} & \textbf{Implicit zero} & \textbf{memset} \\
        \hline
        memset      & -             & -  & No  & Yes \\
        stg         & \texttt{stg}  & 16 & No  & No  \\
        st2g        & \texttt{st2g} & 32 & No  & No  \\
        stgp        & \texttt{stgp} & 16 & Yes & No  \\
        stzg        & \texttt{stzg} & 16 & Yes & No  \\
        stg+memset  & \texttt{stg}  & 16 & No  & Yes \\
        st2g+memset & \texttt{st2g} & 32 & No  & Yes \\
        \hline
    \end{tabular}
    \caption{Benchmarking Variants}
    \label{tab:stg-instructions}
\end{table}

We ran the benchmark on our testbed (see \cref{sec:experimental-setup}) tagging a 128\,MiB memory region.
Before each run, we requested a fresh piece of memory with \texttt{mmap} and ran the specified configuration to prevent interference through already filled CPU caches.

\begin{figure}[h]
    \centering
    \input{plots/stg.pgf}
    \caption{Performance results of the benchmarking variants from \cref{tab:stg-instructions} on 128\,MiB of memory.}
    \label{fig:stg-performance}
\end{figure}

All runs were performed on each type of CPU core on the Pixel 8.
In \cref{fig:stg-performance} we can see the results.
As expected, the instructions setting the memory implicitly to zero are faster than those tagging and then zeroing using an explicit memset.
Both \texttt{stzg} and \texttt{stgp} are only slightly slower than a raw memset, as their memory accesses do not need to perform tag checks~\cite{ARMA2024Arch64}.

Surprisingly, \texttt{st2g} is only slighly faster than \texttt{stg}.
We suspect \texttt{st2g} translates to roughly the same microops as two \texttt{stg} instructions do, except alignment checks and register write-backs, but were not able to confirm that.

\subsection{Synchronous and Asynchronous Mode}
\label{subsec:synchronous-and-asynchronous-mode}

We evaluated the performance of sequential memory accesses with MTE disabled and enabled using synchronous mode and asynchronous mode, on each type of CPU core on the Pixel 8.
In \cref{fig:sync-async-performance} we see that with synchronous mode, \texttt{memset} is 11.5\%, 8.9\%, and 13.2\% slower on the respective cores compared to the baseline with \ac{MTE} disabled.
Asynchronous mode is closer to the baseline with an overhead of 0.9\%, 3.7\%, and 6.1\% respectively.

\begin{figure}[h]
    \centering
    \input{plots/sync-async.pgf}
    \caption{Runtime of \texttt{memset} on 128\,MiB of memory using different \ac{MTE} modes.}
    \label{fig:sync-async-performance}
\end{figure}
