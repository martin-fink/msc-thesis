\chapter{Evaluation}
\label{ch:eval}


\section{Experimental Setup}\label{sec:experimental-setup}

We conducted our benchmarks on a Google Pixel 8 equipped with a Google Tensor G3 chip, comprising 1\,$\times$\,Cortex-X3 (2.91\,GHz), 4\,$\times$\,Cortex-A715 (2.37\,GHz), and 4\,$\times$\,Cortex-A510 (1.7\,GHz) cores, with Memory Tagging Extension (MTE) enabled.
To mitigate thermal throttling, we attached a cooling fan to the device.
Additionally, we pinned benchmarks to the low-power Cortex-A510 cores, which significantly decreased thermal throttling and noise in our benchmarks. \todo{should we leave this in?}
As of the date of writing, this is the sole commercially available device featuring MTE.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{c || c c c}
        \hline
        \textbf{Variant} & \textbf{64-bit} & \textbf{memsafe} & \textbf{bounds-chk} \\
        \hline
        wasm32           & No              & No               & No                  \\
        wasm64           & Yes             & No               & No                  \\
        mem-safety       & Yes             & Yes              & No                  \\
        mte-bounds       & Yes             & No               & Yes                 \\
        all              & Yes             & Yes              & Yes                 \\
        \hline
    \end{tabular}
    \caption{Benchmarking Variants}
    \label{tab:benchmark-variants}
\end{table}

\subsection{Benchmark Variants}\label{subsec:benchmark-variants}
The benchmarks utilized were from the Polybench-C suite.
\todo{add more real-world benchmarks}
The variants compared are detailed in \cref{tab:benchmark-variants}.

\section{Performance Overheads}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{./plots/runtimes}
    \caption{{PolyBench-C runtime overheads of our prototype. (a) Baseline. (b) Relative overhead of wasm64 with memory safety features. (c) Bounds checks implemented using MTE.}{}}
    \label{fig:runtime_overheads}
\end{figure*}

Figure~\ref{fig:runtime_overheads} illustrates the runtime overheads for PolyBench-C benchmarks.
The mean runtime compared to the baseline for the memory safety implementation is 100.8\,\%, with a maximum of 109.4\%.
With memory safety features disabled, and MTE employed for bounds checking, we reach a median runtime of 70.9\% relative to the baseline, with minimum and maximum runtimes of 40.4\% and 93.1\%, respectively.

\todo{combining bounds + memsafe}


\section{Memory Overheads}\label{sec:memory-overheads}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{./plots/mem-overhead}
    \caption{{PolyBench-C memory overheads of our prototype. (a) Baseline. (b) Relative overhead of wasm64 with memory safety features. (c) Bounds checks implemented using MTE.}{}}
    \label{fig:memory_overheads}
\end{figure*}

We measure the memory overhead with GNU time.

Memory tagging incurs overhead, particularly for small allocations due to the 16-byte alignment required for MTE.
The MTE backend further introduces overhead, as observed in Figure \ref{fig:memory_overheads}, due to OS-level memory allocation requirements when MTE is active.
Enabling our memory safety mechanism results in an average overhead of 1\%, with a range of -0.67\% to 23.5\%.
In contrast, replacing bounds checks with MTE, which necessitates tagging the entire WebAssembly linear memory, leads to an average memory overhead of 0.26\%, with a range of -0.7\% to 1.4\%.


\section{Security Guarantees}\label{sec:security-guarantees}


\todo{insert evaluation here}


\section{Overhead of wasm32}
\label{sec:eval-wasm32-wasm64}

\todo{insert evaluation here}


\section{MTE Performance evaluation}
\label{sec:mte-performance-evaluation}

We evaluated the performance of different characteristics of \ac{MTE} as implemented on the Tensor G3 chip.
All the programs used to measure results in this section are implemented in Rust and available on GitHub\footnote{\url{https://github.com/martin-fink/mte-stg-bench}}.
As our benchmarking library we used criterion\footnote{\url{https://github.com/bheisler/criterion.rs}}.
After each benchmarking run, we let the device cool down for 30\,seconds to prevent throttling.

\subsection{Tagging Primitives}
\label{subsec:tagging-primitives}

We evaluated the performance of the different types of instructions to set the tag for a segment of memory available in EL0 (user space) with the combinations described in \cref{tab:stg-instructions}.
Here, instruction refers to the instruction being used to set the tag and granule size refers to the size of memory being tagged with a single instruction.
The instructions \texttt{stzg} and \texttt{stgp} implicitly set the granule to zero, while we have to use an explicit memset for other instructions.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{| l || c | c |  c | c |}
        \hline
        \textbf{Variant} & \textbf{Instruction} & \textbf{Granule size} & \textbf{Implicit zero} & \textbf{memset} \\
        \hline
        memset      & -             & -  & No  & Yes \\
        stg         & \texttt{stg}  & 16 & No  & No  \\
        stgp        & \texttt{stgp} & 16 & Yes & No  \\
        st2g        & \texttt{st2g} & 32 & No  & No  \\
        stzg        & \texttt{stzg} & 16 & Yes & No  \\
        stg+memset  & \texttt{stg}  & 16 & No  & Yes \\
        st2g+memset & \texttt{st2g} & 32 & No  & Yes \\
        \hline
    \end{tabular}
    \caption{Benchmarking Variants}
    \label{tab:stg-instructions}
\end{table}

We ran the benchmark on our testbed (see \cref{sec:experimental-setup}) tagging a 128\,MiB memory region.
Before each run, we requested a fresh piece of memory with \texttt{mmap} and ran the specified configuration to prevent interference through already filled CPU caches.

\begin{figure}[h]
    \centering
    \input{plots/stg.pgf}
    \caption{Performance results of the benchmarking variants from \cref{tab:stg-instructions} on 128\,MiB of memory.}
    \label{fig:stg-performance}
\end{figure}

All runs were performed on each type of CPU core on the Pixel 8.
In \cref{fig:stg-performance} we can see the results.
As expected, the instructions setting the memory implicitly to zero are faster than those tagging and then zeroing using an explicit memset.
Both \texttt{stzg} and \texttt{stgp} are only slightly slower than a raw memset, as their memory accesses do not need to perform tag checks~\cite{ARMA2024Arch64}.

Surprisingly, \texttt{st2g} is only slighly faster than \texttt{stg}.
We suspect \texttt{st2g} translates to roughly the same microops as two \texttt{stg} instructions do, except alignment checks and register write-backs, but were not able to confirm that.

\subsection{Synchronous and Asynchronous Mode}
\label{subsec:synchronous-and-asynchronous-mode}

We evaluated the performance of sequential memory accesses with MTE disabled and enabled using synchronous mode and asynchronous mode, on each type of CPU core on the Pixel 8.
In \cref{fig:sync-async-performance} we see that with synchronous mode, \texttt{memset} is 11.5\%, 8.9\%, and 13.2\% slower on the respective cores compared to the baseline with \ac{MTE} disabled.
Asynchronous mode is closer to the baseline with an overhead of 1\%, 3.7\%, and 6.1\% respectively.

\begin{figure}[h]
    \centering
    \input{plots/sync-async.pgf}
    \caption{Runtime of \texttt{memset} on 128\,MiB of memory using different \ac{MTE} modes.}
    \label{fig:sync-async-performance}
\end{figure}
