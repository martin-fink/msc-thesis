\chapter{Evaluation}
\label{ch:eval}


\section{Experimental Setup}\label{sec:experimental-setup}

We conducted our benchmarks on a Google Pixel 8 equipped with a Google Tensor G3 chip, comprising 1\,$\times$\,Cortex-X3 (2.91\,GHz), 4\,$\times$\,Cortex-A715 (2.37\,GHz), and 4\,$\times$\,Cortex-A510 (1.7\,GHz) cores, with Memory Tagging Extension (MTE) enabled.
As of the time of writing, this is the sole commercially available device featuring MTE.
To mitigate thermal throttling, we attached a cooling fan to the device.
Each performance test was run on each CPU type available on the Tensor G3 chip by pinning to a single respective core.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{c || c|c|c}
        \textbf{Variant} & \textbf{Pointer Width} & \textbf{Memory Safety} & \textbf{MTE Bounds Checks} \\
        \hline
        wasm32     & 32\,bit & No  & No  \\
        wasm64     & 64\,bit & No  & No  \\
        mem-safety & 64\,bit & Yes & No  \\
        mte-bounds & 64\,bit & No  & Yes \\
        combined   & 64\,bit & Yes & Yes \\
    \end{tabular}
    \caption{Runtime Benchmarking Configurations}
    \label{tab:benchmark-variants}
\end{table}

\noindent
We ran the benchmarks from the PolyBench/C 3.2 suite~\cite{polybenchc}.

\section{Performance Overheads}
\label{sec:performance-overheads}

\begin{figure}[ht]
    \centering
    \input{plots/runtimes-all.pgf}
    \caption{PolyBench/C runtime overheads of different configurations described in \cref{tab:benchmark-variants}, normalized to wasm64.}
    \label{fig:runtime-overheads-combined}
\end{figure}

\Cref{fig:runtime-overheads-combined} illustrates the mean runtime overheads for PolyBench/C benchmarks for each CPU core available on the Tensor G3 chip.
We can see that, compared to wasm64, our memory safety extension has a mean overhead of 5.8\,\% and 4.9\,\% on the two out-of-order high performance cores.
On the in-order Cortex-A510, we see an overhead of just 1.0\,\%.
Generally we see that the overhead of bounds checks through the switch from wasm32 to wasm64 is lower on the out-of-order cores, as those can speculate bounds checks, while the in-order cores cannot.
This also explains the low overhead of our memory safety extension on the in-order cores, as we spend more time doing bounds checks compared to the in-order cores.

When we replace software-based bounds checks with \ac{MTE} bounds checks, we see the overhead largely disappearing.
The remaining overhead can be explained through (1) the natural overhead that enabling \ac{MTE} synchronous mode costs (see \cref{subsec:synchronous-and-asynchronous-mode}) and (2) that the linear memory needs to be tagged after allocation.
This overhead is especially noticeable for short-running modules that require large amounts of linear memory.

Combining both \ac{MTE} bounds and \ac{MTE} memory safety, we see a slight increase from just \ac{MTE} bounds, that is smaller than the jump from wasm64 to memory safety.
This is as we have already paid for the \ac{MTE} overhead, the overhead we are seeing here is the overhead resulting in additional tagging instructions.

We could decrease the overhead even further by switching to \ac{MTE} asynchronous mode, which poses a lower overhead, compared to sync mode (see \cref{subsec:synchronous-and-asynchronous-mode}).
However, this dramatically reduces the security guarantees provided by \ac{MTE}, as illegal writes and reads may become observable.
This disqualifies async \ac{MTE} for bounds checking of \ac{WASM} sandboxes, as attackers may carefully craft malicious code to escape their sandbox.
For the memory safety extension, users may decide the additional risk is worth the reduced overhead, e.g., when the memory safety extension is not used as a primary defense mechanism, but as a second layer or to find bugs in the wild.

\section{Memory Overheads}\label{sec:memory-overheads}

Memory tagging incurs overhead, particularly for small allocations due to the 16-byte alignment required for MTE.
In our measurements, we did not see a significant difference in maximum \ac{rss}.
This is because (1) safe allocations do not incur space overhead, and (2) for large allocations, the 16-byte alignment overhead is proportionally small.
The main overhead in \cref{fig:memory-overheads} is mostly due to the switch to wasm64.

\begin{figure*}[t]
    \centering
    \input{plots/mem-overhead.pgf}
    \caption{PolyBench/C memory overheads of different configurations described in \cref{tab:benchmark-variants}, normalized to wasm64.}
    \label{fig:memory-overheads}
\end{figure*}

\section{Security Guarantees}\label{sec:security-guarantees}

We evaluate the security guarantees from the perspective of internal and external memory safety, as defined in \cref{sec:threat-model}.

\subsection{External Memory Safety}
\label{subsec:sec-guarantees-external-memory-safety}

Running with \ac{MTE} bounds checks, with \ac{MTE} being configured to run in synchronous mode prevents programs from escaping their sandbox.
We prevent programs from forging tags by masking the respective tag bits before computing the effective address, as described in \cref{subsec:bounds-checks}.
Other defence mechanisms, such as structured control flow, the typed stack, function calls through typed and checked tables remain unchanged by our implementation.
We do however limit the number of sandboxes in one process to at most 15, which is required to assign a distinct tag to each sandbox.

Switching to async \ac{MTE} mode is not feasible to retain external memory safety, as memory accesses outside the sandbox may become observable both to the \ac{WASM} module performing the illegal access and to other modules.

\subsection{Internal Memory Safety}
\label{subsec:sec-guarantees-internal-memory-safety}

Our choice to leverage \ac{MTE} allows for a low overhead so this can be deployed in addition to testing using other mechanisms, such as \ac{ASAN}.
For internal memory safety, however, our approach does not provide complete memory safety, as \ac{MTE} provides a limited number of tags and should be used as a secondary, not primary defense mechanism to harden applications in the wild.
A tag collision means two memory regions could accidentally share the same tag, potentially leading to a missed security violation.
For example, if a buffer overflow writes slightly beyond its intended bounds, but the overwritten memory has the same tag, \ac{MTE} would not detect the issue.

We can calculate the probability for a tag collision for $k=2$ random tags according to \cref{fig:tag-collision}, with $n=16$ available tags for \ac{MTE} bounds disabled (\cref{fig:tag-collision-16}) and $n=8$ for bounds enabled (\cref{fig:tag-collision-8}), as we reserve one bit for the bounds checking mechanism.

\begin{figure}[h]
    \centering
    \begin{subfigure}[T]{0.45\textwidth}
        \centering
        \begin{align*}
            V_{nr} &= \frac{n!}{(n - k)!} = 240 \\
            V_t &= n^k = 256 \\
            P(\text{c}) &= 1 - \frac{V_{nr}}{V_t} = 6.25\%
        \end{align*}
        \caption{Probability of a tag collision with $n=16$ and $k=2$.}
        \label{fig:tag-collision-16}
    \end{subfigure}
    \hfill
    \begin{subfigure}[T]{0.45\textwidth}
        \centering
        \begin{align*}
            V_{nr} &= \frac{n!}{(n - k)!} = 56 \\
            V_t &= n^k = 64 \\
            P(\text{c}) &= 1 - \frac{V_{nr}}{V_t} = 12.5\%
        \end{align*}
        \caption{Probability of a tag collision with $n=8$ and $k=2$.}
        \label{fig:tag-collision-8}
    \end{subfigure}
    \caption{Probabilities of tag collisions for random tags.}
    \label{fig:tag-collision}
\end{figure}

However, we ensure that adjacent allocations are always tagged with distinct tags, and that memory is tagged with a new tag when it is freed, thus ensuring that spatial errors up to 16\,bytes and temporal errors until the next allocation of a chunk of memory are always caught.

\section{MTE Performance evaluation}
\label{sec:mte-performance-evaluation}

We evaluated the performance of different characteristics of \ac{MTE} as implemented on the Tensor G3 chip.
All the programs used to measure results in this section are implemented in Rust and available on GitHub\footnote{\url{https://github.com/martin-fink/mte-stg-bench}}.
As our benchmarking library we used criterion\footnote{\url{https://github.com/bheisler/criterion.rs}}.
After each benchmarking run, we let the device cool down for 30\,seconds to prevent throttling.

\subsection{Tagging Primitives}
\label{subsec:tagging-primitives}

We evaluated the performance of the different types of instructions to set the tag for a segment of memory available in EL0 (user space) with the combinations described in \cref{tab:stg-instructions}.
Here, instruction refers to the instruction being used to set the tag and granule size refers to the size of memory being tagged with a single instruction.
The instructions \texttt{stzg} and \texttt{stgp} implicitly set the granule to zero, while we have to use an explicit memset for other instructions.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{c || c | c |  c | c }
        \textbf{Variant} & \textbf{Instruction} & \textbf{Granule size} & \textbf{Implicit zero} & \textbf{memset} \\
        \hline
        memset      & -             & -  & No  & Yes \\
        stg         & \texttt{stg}  & 16 & No  & No  \\
        st2g        & \texttt{st2g} & 32 & No  & No  \\
        stgp        & \texttt{stgp} & 16 & Yes & No  \\
        stzg        & \texttt{stzg} & 16 & Yes & No  \\
        stg+memset  & \texttt{stg}  & 16 & No  & Yes \\
        st2g+memset & \texttt{st2g} & 32 & No  & Yes \\
    \end{tabular}
    \caption{MTE Benchmarking Variants}
    \label{tab:stg-instructions}
\end{table}

We ran the benchmark on our testbed (see \cref{sec:experimental-setup}) tagging a 128\,MiB memory region.
Before each run, we requested a fresh piece of memory with \texttt{mmap} and ran the specified configuration to prevent interference through already filled CPU caches.

\begin{figure}[h]
    \centering
    \input{plots/stg.pgf}
    \caption{Performance results of the benchmarking variants from \cref{tab:stg-instructions} on 128\,MiB of memory.}
    \label{fig:stg-performance}
\end{figure}

All runs were performed on each type of CPU core on the Pixel 8.
In \cref{fig:stg-performance} we can see the results.
As expected, the instructions setting the memory implicitly to zero are faster than those tagging and then zeroing using an explicit memset.
Both \texttt{stzg} and \texttt{stgp} are only slightly slower than a raw memset, as their memory accesses do not need to perform tag checks~\cite{ARMA2024Arch64}.

Surprisingly, \texttt{st2g} is only slighly faster than \texttt{stg}.
We suspect \texttt{st2g} translates to roughly the same microops as two \texttt{stg} instructions do, except alignment checks and register write-backs, but were not able to confirm that.

\subsection{Synchronous and Asynchronous Mode}
\label{subsec:synchronous-and-asynchronous-mode}

We evaluated the performance of sequential memory accesses with MTE disabled and enabled using synchronous mode and asynchronous mode, on each type of CPU core on the Pixel 8.
In \cref{fig:sync-async-performance} we see that with synchronous mode, \texttt{memset} is 11.5\%, 8.9\%, and 13.2\% slower on the respective cores compared to the baseline with \ac{MTE} disabled.
Asynchronous mode is closer to the baseline with an overhead of 0.9\%, 3.7\%, and 6.1\% respectively.

\begin{figure}[h]
    \centering
    \input{plots/sync-async.pgf}
    \caption{Runtime of \texttt{memset} on 128\,MiB of memory using different \ac{MTE} modes.}
    \label{fig:sync-async-performance}
\end{figure}

\subsection{Migrating Tagged Memory}
\label{subsec:migrating-tagged-memory}

Migrating tagged memory involves the coordinated transfer of both data and its associated type information.
In \cref{fig:migrate-performance}, we analyze the performance of various migration strategies.

\begin{enumerate}
    \item Baseline (\texttt{memcpy} with MTE): This baseline establishes the cost of a standard memory copy operation with \ac{MTE} enabled.
    \item MTE Disable/Re-enable: Disabling MTE, copying data with \texttt{memcpy}, transferring tags, and re-enabling MTE.
    This method temporarily compromises memory safety during the transfer if other threads rely on \ac{MTE} being active during this approach.
    \item Iterative Copying: Copying tags and data in 16\,byte granules allows copying tags and data simultaneous, while keeping \ac{MTE} active for other threads.
\end{enumerate}

Approach 3 is slightly slower than approach 2 on the Cortex-X3, but faster on the other cores.

\begin{figure}[h]
    \centering
    \input{plots/migrate.pgf}
    \caption{Runtime comparison of strategies for moving 128 MiB of memory and tags between regions.}
    \label{fig:migrate-performance}
\end{figure}
